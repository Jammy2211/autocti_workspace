{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Modeling: Start Here\n",
        "====================\n",
        "\n",
        "This script is the starting point for modeling of 1D CTI datasets and it provides an overview \n",
        "of the modeling API.\n",
        "\n",
        "After reading this script, the `features`, `customize` and `searches` folders provide example for performing CTI\n",
        "modeling in different ways and customizing the analysis.\n",
        "\n",
        "__Model__\n",
        "\n",
        "In this script, we will fit a 1D CTI Dataset to calibrate a CTI model, where:\n",
        "\n",
        " - The CTI model consists of multiple parallel `TrapInstantCapture` species.\n",
        " - The `CCD` volume filling is a simple parameterization with just a `well_fill_power` parameter.\n",
        " \n",
        " __Plotters__\n",
        "\n",
        "To produce images of the data `Plotter` objects are used, which are high-level wrappers of matplotlib\n",
        "code which produce high quality visualization of strong CTIes.\n",
        "\n",
        "The `PLotter` API is described in the script `autoCTI_workspace/*/plot/start_here.py`.\n",
        "\n",
        "__Simulation__\n",
        "\n",
        "This script fits a simulated `Imaging` dataset of a strong CTI, which is produced in the\n",
        "script `autoCTI_workspace/*/imaging/simulators/start_here.py`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "from os import path\n",
        "import autofit as af\n",
        "import autocti as ac\n",
        "import autocti.plot as aplt"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Dataset__\n",
        "\n",
        "We begin by loading the CTI dataset `species_x2` via .fits files, which is a data format used by astronomers to \n",
        "store images.\n",
        "\n",
        "A number of steps are performed below which prepare us to load the dataset beforehand. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_name = \"simple\"\n",
        "dataset_path = path.join(\"dataset\", \"dataset_1d\", dataset_name)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Shape__\n",
        "\n",
        "The 1D shape of each 1D dataset, where the 1D dataset we will load is 200 pixels long."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "shape_native = (200,)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Regions__\n",
        "\n",
        "Use `Region1D` objects to define the locations of the prescan and overscan on the 1D data. \n",
        "\n",
        "1D regions are defined as a tuple of the form (x0, x1) = (left-pixel, right-pixel), where the integer values of this\n",
        "tuple are used to perform NumPy array indexing of the 1D data.\n",
        "\n",
        "For example, if the overscan of 1D data is between pixels 40 and 50, its region is `region=(40, 50)`.\n",
        "\n",
        "These are used to visualize these regions of the 1D CTI dataset during the model-fit and customize aspects of the \n",
        "model-fit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "prescan = ac.Region1D(region=(0, 10))\n",
        "overscan = ac.Region1D(region=(190, 200))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__FPR / EPER__\n",
        "\n",
        "Specify the charge regions on the 1D CTI Dataset, corresponding to where an injected signal is present that has its \n",
        "electrons captured and trailed by CTI.\n",
        "\n",
        "This is referred to as a the \"First Pixel Response\" (FPR), with the trails of electrons which appear after it \n",
        "referred to as the \"Extended Pixel Edge Response\" (EPER).\n",
        "\n",
        "This dataset has only one charge region (which is between pixels 10 and 20), but more regions can be specified if\n",
        "required."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "region_list = [(10, 20)]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Normalizations__\n",
        "\n",
        "Specify the normalization of the charge in every individual 1D CTI dataset. \n",
        "\n",
        "This is not used internally by **PyAutoCTI**, and only required for loading the dataset because the dataset file\n",
        "names use the normalizations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "norm_list = [100, 5000, 25000, 200000]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The total number of 1D CTI datasets that are fitted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "total_datasets = len(norm_list)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Layout__\n",
        "\n",
        "We now create a `Layout1D` object for every 1D dataset fitted in this script.\n",
        "\n",
        "This object contains all functionality associated with the layout of the data (e.g. where the FPR is, where the\n",
        "EPERs are, where the overscans are, etc.). \n",
        "\n",
        "This is used for performing tasks like extracting a small region of the data for visualization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "layout_list = [\n",
        "    ac.Layout1D(\n",
        "        shape_1d=shape_native,\n",
        "        region_list=region_list,\n",
        "        prescan=prescan,\n",
        "        overscan=overscan,\n",
        "    )\n",
        "    for i in range(total_datasets)\n",
        "]\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Dataset__\n",
        "\n",
        "We now use a `Dataset1D` object to load every 1D CTI dataset, including a noise-map and pre-cti data containing the d\n",
        "ata before read-out and therefore without CTI. \n",
        "\n",
        "The `pixel_scales` define the arc-second to pixel conversion factor of the image, which for the dataset we are using \n",
        "is 0.1\" / pixel. This is used for visualization only, specifically to convert axis labels from pixels to arc-seconds."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_list = [\n",
        "    ac.Dataset1D.from_fits(\n",
        "        data_path=path.join(dataset_path, f\"norm_{int(norm)}\", \"data.fits\"),\n",
        "        noise_map_path=path.join(dataset_path, f\"norm_{int(norm)}\", \"noise_map.fits\"),\n",
        "        pre_cti_data_path=path.join(\n",
        "            dataset_path, f\"norm_{int(norm)}\", \"pre_cti_data.fits\"\n",
        "        ),\n",
        "        layout=layout,\n",
        "        pixel_scales=0.1,\n",
        "    )\n",
        "    for layout, norm in zip(layout_list, norm_list)\n",
        "]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use a `Dataset1DPlotter` to the plot the data, including: \n",
        "\n",
        " - `data`: The 1D CTI data.\n",
        " - `noise_map`: The noise-map of the data, which quantifies the noise in every pixel as their RMS values.\n",
        " - `pre_cti_data`: The data before CTI, which has CTI added to it for every CTI model, which is compared to the data. \n",
        " - `signal_to_noise_map`: Quantifies the signal-to-noise in every pixel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_plotter = aplt.Dataset1DPlotter(dataset=dataset_list[0])\n",
        "dataset_plotter.subplot_dataset()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Mask__\n",
        "\n",
        "We apply a `Mask1D` to the dataset, which defines the regions of the data we fit the CTI model to the data. \n",
        "\n",
        "We mask the FPR of each dataset, such that this fit will only the EPER to calibrate the CTI model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "mask = ac.Mask1D.all_false(\n",
        "    shape_slim=dataset_list[0].shape_slim,\n",
        "    pixel_scales=dataset_list[0].pixel_scales,\n",
        ")\n",
        "\n",
        "mask = ac.Mask1D.masked_fpr_and_eper_from(\n",
        "    mask=mask,\n",
        "    layout=dataset_list[0].layout,\n",
        "    settings=ac.SettingsMask1D(fpr_pixels=(0, 10)),\n",
        "    pixel_scales=dataset_list[0].pixel_scales,\n",
        ")\n",
        "\n",
        "dataset_list = [dataset.apply_mask(mask=mask) for dataset in dataset_list]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By plotting the masked data, the mask removes the FPR of the data and now shows only the EPER trails."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_plotter = aplt.Dataset1DPlotter(dataset=dataset_list[0])\n",
        "dataset_plotter.subplot_dataset()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Clocker / arCTIc__\n",
        "\n",
        "To model the CCD clocking process, including CTI, we use  arCTIc, or the \"algorithm for Charge Transfer Inefficiency \n",
        "clocking\".\n",
        "\n",
        "arCTIc is written in c++ can be used standalone outside of **PyAutoCTI** as described on its GitHub \n",
        "page (https://github.com/jkeger/arctic). **PyAutoCTI** uses arCTIc's built-in Python wrapper.\n",
        "\n",
        "In **PyAutoCTI** we call arCTIc via a `Clocker` object, which is a Python class that wraps arCTIc. This class has \n",
        "many optional inputs that customize how clocking is performed, but we'll omit these for now to keep things simple.\n",
        "\n",
        "For this example, we only input the `express` parameter, which determines how many electrons are clocked per cycle\n",
        "and trades off speed for accuracy. For this example we use `express=5`, which is a good balance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "clocker = ac.Clocker1D(express=5)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model__\n",
        "\n",
        "We now compose our CTI model, which represents the trap species and CCD volume filling behaviour used to fit the CTI \n",
        "1D data. In this example we fit a CTI model with:\n",
        "\n",
        " - Two `TrapInstantCapture`'s which capture electrons during clocking instantly in the parallel direction\n",
        " [4 parameters].\n",
        " \n",
        " - A simple `CCD` volume filling parametrization with fixed notch depth and capacity [1 parameter].\n",
        "\n",
        "The number of free parameters and therefore the dimensionality of non-linear parameter space is N=5.\n",
        "\n",
        "__Model Composition__\n",
        "\n",
        "The API below for composing a CTI model uses the `Model` and `Collection` objects, which are imported from \n",
        "**PyAutoCTI**'s parent project **PyAutoFit** \n",
        "\n",
        "The API is fairly self explanatory and is straight forward to extend, for example adding more light profiles\n",
        "to the CTI and source or using a different mass profile.\n",
        "\n",
        "__Model Cookbook__\n",
        "\n",
        "A full description of model composition, including CTI model customization, is provided by the model cookbook: \n",
        "\n",
        "https://pyautocti.readthedocs.io/en/latest/general/model_cookbook.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "trap_0 = af.Model(ac.TrapInstantCapture)\n",
        "trap_1 = af.Model(ac.TrapInstantCapture)\n",
        "\n",
        "trap_0.add_assertion(trap_0.release_timescale < trap_1.release_timescale)\n",
        "\n",
        "trap_list = [trap_0, trap_1]\n",
        "\n",
        "ccd = af.Model(ac.CCDPhase)\n",
        "ccd.well_notch_depth = 0.0\n",
        "ccd.full_well_depth = 200000.0\n",
        "\n",
        "model = af.Collection(cti=af.Model(ac.CTI1D, trap_list=trap_list, ccd=ccd))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `info` attribute shows the model in a readable format.\n",
        "\n",
        "The `info` below may not display optimally on your computer screen, for example the whitespace between parameter\n",
        "names on the left and parameter priors on the right may lead them to appear across multiple lines. This is a\n",
        "common issue in Jupyter notebooks.\n",
        "\n",
        "The`info_whitespace_length` parameter in the file `config/general.yaml` in the [output] section can be changed to \n",
        "increase or decrease the amount of whitespace (The Jupyter notebook kernel will need to be reset for this change to \n",
        "appear in a notebook)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(model.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Search__\n",
        "\n",
        "The CTI model is fitted to the data using a non-linear search. \n",
        "\n",
        "All examples in the autoCTI workspace use the nested sampling algorithm \n",
        "Nautilus (https://nautilus.readthedocs.io/en/latest/), which extensive testing has revealed gives the most accurate\n",
        "and efficient CTI modeling results.\n",
        "\n",
        "We make the following changes to the Nautilus settings:\n",
        "\n",
        " - Increase the number of live points, `nlive`, from the default value of 50 to 100. \n",
        " - Increase the number of random walks per live point, `walks` from the default value of 5 to 10. \n",
        "\n",
        "These are the two main Nautilus parameters that trade-off slower run time for a more reliable and accurate fit.\n",
        "Increasing both of these parameter produces a more reliable fit at the expense of longer run-times.\n",
        "\n",
        "__Customization__\n",
        "\n",
        "The folders `autoCTI_workspace/*/imaging/modeling/searches` gives an overview of alternative non-linear searches,\n",
        "other than Nautilus, that can be used to fit CTI models. They also provide details on how to customize the\n",
        "model-fit, for example the priors.\n",
        "\n",
        "The `name` and `path_prefix` below specify the path where results ae stored in the output folder:  \n",
        "\n",
        " `/autoCTI_workspace/output/imaging/modeling/simple/light[bulge_disk]_mass[sie]_source[bulge]/unique_identifier`.\n",
        "\n",
        "__Unique Identifier__\n",
        "\n",
        "In the path above, the `unique_identifier` appears as a collection of characters, where this identifier is generated \n",
        "based on the model, search and dataset that are used in the fit.\n",
        " \n",
        "An identical combination of model and search generates the same identifier, meaning that rerunning the script will use \n",
        "the existing results to resume the model-fit. In contrast, if you change the model or search, a new unique identifier \n",
        "will be generated, ensuring that the model-fit results are output into a separate folder.\n",
        "\n",
        "We additionally want the unique identifier to be specific to the dataset fitted, so that if we fit different datasets\n",
        "with the same model and search results are output to a different folder. We achieve this below by passing \n",
        "the `dataset_name` to the search's `unique_tag`.\n",
        "\n",
        "__Number Of Cores__\n",
        "\n",
        "We include an input `number_of_cores`, which when above 1 means that Nautilus uses parallel processing to sample multiple \n",
        "CTI models at once on your CPU. When `number_of_cores=2` the search will run roughly two times as\n",
        "fast, for `number_of_cores=3` three times as fast, and so on. The downside is more cores on your CPU will be in-use\n",
        "which may hurt the general performance of your computer.\n",
        "\n",
        "You should experiment to figure out the highest value which does not give a noticeable loss in performance of your \n",
        "computer. If you know that your processor is a quad-core processor you should be able to use `number_of_cores=4`. \n",
        "\n",
        "Above `number_of_cores=4` the speed-up from parallelization diminishes greatly. We therefore recommend you do not\n",
        "use a value above this.\n",
        "\n",
        "For users on a Windows Operating system, using `number_of_cores>1` may lead to an error, in which case it should be \n",
        "reduced back to 1 to fix it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "search = af.Nautilus(\n",
        "    path_prefix=path.join(\"dataset_1d\", dataset_name), name=\"species[x2]\", n_live=100\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Analysis__\n",
        "\n",
        "We next create an `AnalysisImaging` object, which can be given many inputs customizing how the CTI model is \n",
        "fitted to the data (in this example they are omitted for simplicity).\n",
        "\n",
        "Internally, this object defines the `log_likelihood_function` used by the non-linear search to fit the model to \n",
        "the `Imaging` dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis_list = [\n",
        "    ac.AnalysisDataset1D(dataset=dataset, clocker=clocker) for dataset in dataset_list\n",
        "]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By summing this list of analysis objects, we create an overall `Analysis` which we can use to fit the CTI model, where:\n",
        "\n",
        " - The log likelihood function of this summed analysis class is the sum of the log likelihood functions of each \n",
        " individual analysis object.\n",
        "\n",
        " - The summing process ensures that tasks such as outputting results to hard-disk, visualization, etc use a \n",
        " structure that separates each analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis = sum(analysis_list)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can parallelize the likelihood function of these analysis classes, whereby each evaluation will be performed on a \n",
        "different CPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis.n_cores = 1"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model-Fit__\n",
        "\n",
        "We can now begin the model-fit by passing the model and analysis object to the search, which performs the \n",
        "nautilus non-linear search in order to find which models fit the data with the highest likelihood.\n",
        "\n",
        "__Output Folder__\n",
        "\n",
        "Now this is running you should checkout the `autoCTI_workspace/output` folder.\n",
        "\n",
        "This is where the results of the search are written to your hard-disk (in the `tutorial_1_non_linear_search` folder). \n",
        "When its completed, images, results and information about the fit appear in this folder, meaning that you don't need \n",
        "to keep running Python code to see the result.\n",
        "\n",
        "__On The Fly Outputs__\n",
        "\n",
        "Even when the search is running, information about the highest likelihood model inferred by the search so far \n",
        "is output to this folder on-the-fly. \n",
        "\n",
        "If you navigate to the folder: \n",
        "\n",
        " `output/dataset_1d/simple` \n",
        " \n",
        "Even before the search has finished, you will see:\n",
        "\n",
        " 1) The `images` folder, where images of the highest likelihood CTI model are output on-the-fly. This includes the\n",
        " `FitImaging` subplot we plotted in the previous chapter, which therefore gives a real sense of 'how good' the model\n",
        " fit is.\n",
        " \n",
        " 2) The `samples` folder, which contains a `.csv` table of every sample of the non-linear search as well as other \n",
        " information. \n",
        " \n",
        " 3) The `model.info` file, which lists the CTI model, its parameters and their priors (discussed in the next tutorial).\n",
        " \n",
        " 4) The `model.results` file, which lists the highest likelihood CTI model and the most probable CTI model with \n",
        " errors (this outputs on-the-fly).\n",
        " \n",
        " 5) The `search.summary` file, which provides a summary of the non-linear search settings and statistics on how well\n",
        " it is performing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "result_list = search.fit(model=model, analysis=analysis)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Result__\n",
        "\n",
        "The search returns a result object, which whose `info` attribute shows the result in a readable format.\n",
        "\n",
        "[Above, we discussed that the `info_whitespace_length` parameter in the config files could b changed to make \n",
        "the `model.info` attribute display optimally on your computer. This attribute also controls the whitespace of the\n",
        "`result.info` attribute.]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(result_list.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `Result` object also contains:\n",
        "\n",
        " - The model corresponding to the maximum log likelihood solution in parameter space.\n",
        " - The corresponding maximum log likelihood `CTI1D` and `FitDataset1D` objects.\n",
        " - Information on the posterior as estimated by the `Nautilus` non-linear search. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(result_list[0].max_log_likelihood_instance.cti.trap_list[0].density)\n",
        "print(result_list[0].max_log_likelihood_instance.cti.ccd.well_fill_power)\n",
        "\n",
        "for result in result_list:\n",
        "    fit_plotter = aplt.FitDataset1DPlotter(fit=result.max_log_likelihood_fit)\n",
        "    fit_plotter.subplot_fit()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Checkout `autocti_workspace/*/dataset_1d/modeling/results.py` for a full description of the result object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}