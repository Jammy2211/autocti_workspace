{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Results: Start Here\n",
        "===================\n",
        "\n",
        "This script is the starting point for investigating the results of modeling and it provides\n",
        "an overview of the modeling API.\n",
        "\n",
        "After reading this script, the `examples` folder provides more detailed examples for analysing the different aspects of\n",
        "performing modeling results outlined here.\n",
        "\n",
        "__Model__\n",
        "\n",
        "We begin by fitting a quick model to a simple dataset, which we will use to illustrate the modeling\n",
        "results API.\n",
        "\n",
        "If you are not familiar with the modeling API and process, checkout the `autocti_workspace/imaging_ci/modeling`\n",
        "folder for examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "from os import path\n",
        "import autofit as af\n",
        "import autocti as ac\n",
        "import autocti.plot as aplt"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model Fit__\n",
        "\n",
        "The code below performs a model-fit using nautilus. \n",
        "\n",
        "You should be familiar with modeling already, if not read the `modeling/start_here.py` script before reading this one!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_name = \"simple\"\n",
        "dataset_path = path.join(\"dataset\", \"dataset_1d\", dataset_name)\n",
        "\n",
        "shape_native = (200,)\n",
        "\n",
        "prescan = ac.Region1D(region=(0, 10))\n",
        "overscan = ac.Region1D(region=(190, 200))\n",
        "\n",
        "region_list = [(10, 20)]\n",
        "\n",
        "norm_list = [100, 5000, 25000, 200000]\n",
        "\n",
        "total_datasets = len(norm_list)\n",
        "\n",
        "layout_list = [\n",
        "    ac.Layout1D(\n",
        "        shape_1d=shape_native,\n",
        "        region_list=region_list,\n",
        "        prescan=prescan,\n",
        "        overscan=overscan,\n",
        "    )\n",
        "    for i in range(total_datasets)\n",
        "]\n",
        "\n",
        "dataset_list = [\n",
        "    ac.Dataset1D.from_fits(\n",
        "        data_path=path.join(dataset_path, f\"norm_{int(norm)}\", \"data.fits\"),\n",
        "        noise_map_path=path.join(dataset_path, f\"norm_{int(norm)}\", \"noise_map.fits\"),\n",
        "        pre_cti_data_path=path.join(\n",
        "            dataset_path, f\"norm_{int(norm)}\", \"pre_cti_data.fits\"\n",
        "        ),\n",
        "        layout=layout,\n",
        "        pixel_scales=0.1,\n",
        "    )\n",
        "    for layout, norm in zip(layout_list, norm_list)\n",
        "]\n",
        "\n",
        "mask = ac.Mask1D.all_false(\n",
        "    shape_slim=dataset_list[0].shape_slim,\n",
        "    pixel_scales=dataset_list[0].pixel_scales,\n",
        ")\n",
        "\n",
        "mask = ac.Mask1D.masked_fpr_and_eper_from(\n",
        "    mask=mask,\n",
        "    layout=dataset_list[0].layout,\n",
        "    settings=ac.SettingsMask1D(fpr_pixels=(0, 10)),\n",
        "    pixel_scales=dataset_list[0].pixel_scales,\n",
        ")\n",
        "\n",
        "dataset_list = [dataset.apply_mask(mask=mask) for dataset in dataset_list]\n",
        "\n",
        "clocker = ac.Clocker1D(express=5)\n",
        "\n",
        "trap_0 = af.Model(ac.TrapInstantCapture)\n",
        "trap_1 = af.Model(ac.TrapInstantCapture)\n",
        "\n",
        "trap_0.add_assertion(trap_0.release_timescale < trap_1.release_timescale)\n",
        "\n",
        "trap_list = [trap_0, trap_1]\n",
        "\n",
        "ccd = af.Model(ac.CCDPhase)\n",
        "ccd.well_notch_depth = 0.0\n",
        "ccd.full_well_depth = 200000.0\n",
        "\n",
        "model = af.Collection(cti=af.Model(ac.CTI1D, trap_list=trap_list, ccd=ccd))\n",
        "\n",
        "search = af.Nautilus(\n",
        "    path_prefix=path.join(\"dataset_1d\", dataset_name), name=\"species[x2]\", n_live=100\n",
        ")\n",
        "\n",
        "analysis_list = [\n",
        "    ac.AnalysisDataset1D(dataset=dataset, clocker=clocker) for dataset in dataset_list\n",
        "]\n",
        "\n",
        "analysis = sum(analysis_list)\n",
        "\n",
        "result_list = search.fit(model=model, analysis=analysis)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Info__\n",
        "\n",
        "As seen throughout the workspace, the `info` attribute shows the result in a readable format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(result_list.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Loading From Hard-disk__\n",
        "\n",
        "When performing fits which output results to hard-disk, a `files` folder is created containing .json / .csv files of \n",
        "the model, samples, search, etc. You should check it out now for a completed fit on your hard-disk if you have\n",
        "not already!\n",
        "\n",
        "These files can be loaded from hard-disk to Python variables via the aggregator, making them accessible in a \n",
        "Python script or Jupyter notebook. They are loaded as the internal **PyAutoFit** objects we are familiar with,\n",
        "for example the `model` is loaded as the `Model` object we passed to the search above.\n",
        "\n",
        "Below, we will access these results using the aggregator's `values` method. A full list of what can be loaded is\n",
        "as follows:\n",
        "\n",
        " - `model`: The `model` defined above and used in the model-fit (`model.json`).\n",
        " - `search`: The non-linear search settings (`search.json`).\n",
        " - `samples`: The non-linear search samples (`samples.csv`).\n",
        " - `samples_info`: Additional information about the samples (`samples_info.json`).\n",
        " - `samples_summary`: A summary of key results of the samples (`samples_summary.json`).\n",
        " - `info`: The info dictionary passed to the search (`info.json`).\n",
        " - `covariance`: The inferred covariance matrix (`covariance.csv`).\n",
        " - `data`: The 1D noisy data used that is fitted (`data.json`).\n",
        " - `noise_map`: The 1D noise-map fitted (`noise_map.json`).\n",
        "\n",
        "The `samples` and `samples_summary` results contain a lot of repeated information. The `samples` result contains\n",
        "the full non-linear search samples, for example every parameter sample and its log likelihood. The `samples_summary`\n",
        "contains a summary of the results, for example the maximum log likelihood model and error estimates on parameters\n",
        "at 1 and 3 sigma confidence.\n",
        "\n",
        "Accessing results via the `samples_summary` is much faster, because as it does not reperform calculations using the full \n",
        "list of samples. Therefore, if the result you want is accessible via the `samples_summary` you should use it\n",
        "but if not you can revert to the `samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from autofit.aggregator.aggregator import Aggregator\n",
        "\n",
        "agg = Aggregator.from_directory(\n",
        "    directory=path.join(\"output\", \"cookbook_result\"),\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Generators__\n",
        "\n",
        "Before using the aggregator to inspect results, lets discuss Python generators. \n",
        "\n",
        "A generator is an object that iterates over a function when it is called. The aggregator creates all of the objects \n",
        "that it loads from the database as generators (as opposed to a list, or dictionary, or another Python type).\n",
        "\n",
        "This is because generators are memory efficient, as they do not store the entries of the database in memory \n",
        "simultaneously. This contrasts objects like lists and dictionaries, which store all entries in memory all at once. \n",
        "If you fit a large number of datasets, lists and dictionaries will use a lot of memory and could crash your computer!\n",
        "\n",
        "Once we use a generator in the Python code, it cannot be used again. To perform the same task twice, the \n",
        "generator must be remade it. This cookbook therefore rarely stores generators as variables and instead uses the \n",
        "aggregator to create each generator at the point of use.\n",
        "\n",
        "To create a generator of a specific set of results, we use the `values` method. This takes the `name` of the\n",
        "object we want to create a generator of, for example inputting `name=samples` will return the results `Samples`\n",
        "object (which is illustrated in detail below)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for samples in agg.values(\"samples\"):\n",
        "    print(samples.parameter_lists[0])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From here on we will use attributes contained in the `result` passed from the `search.fit` method above, as opposed\n",
        "to using the aggregator. This is because things will run faster, but all of the results we use can be loaded using\n",
        "the aggregator as shown above.\n",
        "\n",
        "__Samples__\n",
        "\n",
        "The result's `Samples` object contains the complete set of non-linear search nautilus samples, where each sample \n",
        "corresponds to a set of model parameters that were evaluated and accepted. \n",
        "\n",
        "The examples script `autocti_workspace/*/dataset_1d/results/examples/samples.py` provides a detailed description of \n",
        "this object, including:\n",
        "\n",
        " - Extracting the maximum likelihood model.\n",
        " - Using marginalized PDFs to estimate errors on the model parameters.\n",
        " - Deriving errors on derived quantities, such as the Einstein radius.\n",
        "\n",
        "Below, is an example of how to use the `Samples` object to estimate the mass model parameters which are \n",
        "the median of the probability distribution function and its errors at 3 sigma confidence intervals."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "samples = result_list.samples\n",
        "\n",
        "median_pdf_instance = samples.median_pdf()\n",
        "\n",
        "print(\"Median PDF Model Instances: \\n\")\n",
        "print(median_pdf_instance.cti.trap_list[0])\n",
        "print()\n",
        "\n",
        "ue3_instance = samples.values_at_upper_sigma(sigma=3.0)\n",
        "le3_instance = samples.values_at_lower_sigma(sigma=3.0)\n",
        "\n",
        "print(\"Errors Instances: \\n\")\n",
        "print(ue3_instance.cti.trap_list[0], \"\\n\")\n",
        "print(le3_instance.cti.trap_list[0], \"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Fits__\n",
        "\n",
        "The result's maximum likelihood `FitDataset1D` object contains everything necessary to inspect the model fit to the \n",
        "data.\n",
        "\n",
        "The examples script `autocti_workspace/*/dataset_1d/results/examples/fits.py` provides a detailed description of this \n",
        "object, including:\n",
        "\n",
        " - How to inspect the residuals, chi-squared, likelihood and other quantities.\n",
        " - Outputting resulting images (e.g. the CTI corrected data) to hard-disk.\n",
        " - Refitting the data with other models from the `Samples` object, to investigate how sensitive the fit is to\n",
        "   different models.\n",
        "\n",
        "Below, is an example of how to use the `FitDataset1D` object to output the CTI reconstruction to print the \n",
        "chi-squared and log likelihood values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fit = result_list[0].max_log_likelihood_fit\n",
        "\n",
        "print(fit.chi_squared)\n",
        "print(fit.log_likelihood)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__CTI__\n",
        "\n",
        "The CTI model can be inspected to quantify whether certain science requirements are met by the quality of model.\n",
        "\n",
        "The examples script `autocti_workspace/*/dataset_1d/results/examples/cti.py` describes this, including:\n",
        "\n",
        " - How to translate the inferred CTI model errors to the spurious elliticity of galaxy weak lensing measurements\n",
        "   due to CTI. \n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}