{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Database: Introduction\n",
        "======================\n",
        "\n",
        "The default behaviour of **PyAutoCTI** is for model-fitting results to be output to hard-disc in folders, which are\n",
        "straight forward to navigate and manually check. For small model-fitting tasks this is sufficient, however many users \n",
        "have a need to perform many model fits to large samples of CTI data, making manual inspection of results time consuming.\n",
        "#\n",
        "PyAutoCTI's database feature outputs all model-fitting results as a\n",
        "sqlite3 (https://docs.python.org/3/library/sqlite3.html) relational database, such that all results\n",
        "can be efficiently loaded into a Jupyter notebook or Python script for inspection, analysis and interpretation. This\n",
        "database supports advanced querying, so that specific model-fits (e.g., which fit a certain model or dataset) can be\n",
        "loaded.\n",
        "\n",
        "This script fits a sample of three simulated CTI datasets using the same non-linear search. The results will be used\n",
        "to illustrate the database in the database tutorials that follow.\n",
        "\n",
        "__Model__\n",
        "\n",
        "In this script, we fit a 1D CTI Dataset to calibrate a CTI model, where:\n",
        "\n",
        " - The CTI model consists of multiple parallel `TrapInstantCapture` species.\n",
        " - The `CCD` volume filling is a simple parameterization with just a `well_fill_power` parameter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "import json\n",
        "from os import path\n",
        "import os\n",
        "import autofit as af\n",
        "import autocti as ac"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Dataset__\n",
        "\n",
        "For each dataset we load it from hard-disc, set up its `Analysis` class and fit it with a non-linear search. \n",
        "\n",
        "We want each results to be stored in the database with an entry specific to the dataset. We'll fit 3 different\n",
        "`simple` datasets separately 3 times in order to do this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_name_list = [\"simple\", \"simple\", \"simple\"]\n",
        "\n",
        "norm_list = [100, 5000, 25000, 200000]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Dataset Quantities__\n",
        "\n",
        "The standard quantities we use to load datasets before model-fitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pixel_scales = 0.1\n",
        "shape_native = (200,)\n",
        "prescan = ac.Region1D(region=(0, 10))\n",
        "overscan = ac.Region1D(region=(190, 200))\n",
        "region_list = [(10, 20)]\n",
        "\n",
        "total_datasets = len(norm_list)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Results From Hard Disk__\n",
        "\n",
        "In this example, results will be first be written to hard-disk using the standard output directory structure and we\n",
        "will then build the database from these results. This behaviour is governed by us inputting `session=None`.\n",
        "\n",
        "If you have existing results you wish to build a database for, you can therefore adapt this example you to do this.\n",
        "\n",
        "Later in this example we show how results can also also be output directly to an .sqlite database, saving on hard-disk \n",
        "space. This will be acheived by setting `session` to something that is not `None`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "session = None\n",
        "\n",
        "for i, dataset_name in enumerate(dataset_name_list):\n",
        "    \"\"\"\n",
        "    __Paths__\n",
        "\n",
        "    Set up the config and output paths.\n",
        "    \"\"\"\n",
        "    dataset_path = path.join(\"dataset\", \"dataset_1d\", dataset_name)\n",
        "\n",
        "    \"\"\"\n",
        "    __Layout__\n",
        "\n",
        "    We use the regions above to create the `Layout1D` of every 1D CTI dataset we fit. This is used  for visualizing \n",
        "    the model-fit.\n",
        "    \"\"\"\n",
        "    layout_list = [\n",
        "        ac.Layout1D(\n",
        "            shape_1d=shape_native,\n",
        "            region_list=region_list,\n",
        "            prescan=prescan,\n",
        "            overscan=overscan,\n",
        "        )\n",
        "        for i in range(total_datasets)\n",
        "    ]\n",
        "\n",
        "    \"\"\"\n",
        "    __Dataset__\n",
        "    \n",
        "    We now load every cti-dataset, including a noise-map and pre-cti data containing the data before read-out and\n",
        "    therefore without CTI.\n",
        "    \"\"\"\n",
        "    dataset_list = [\n",
        "        ac.Dataset1D.from_fits(\n",
        "            data_path=path.join(dataset_path, f\"norm_{int(norm)}\", \"data.fits\"),\n",
        "            noise_map_path=path.join(\n",
        "                dataset_path, f\"norm_{int(norm)}\", \"noise_map.fits\"\n",
        "            ),\n",
        "            pre_cti_data_path=path.join(\n",
        "                dataset_path, f\"norm_{int(norm)}\", \"pre_cti_data.fits\"\n",
        "            ),\n",
        "            layout=layout,\n",
        "            pixel_scales=0.1,\n",
        "        )\n",
        "        for layout, norm in zip(layout_list, norm_list)\n",
        "    ]\n",
        "\n",
        "    \"\"\"\n",
        "    __Mask__\n",
        "\n",
        "    We now mask every 1D dataset, removing the FPR of each dataset so we use only the EPER to calibrate the CTI model.\n",
        "    \"\"\"\n",
        "    mask = ac.Mask1D.all_false(\n",
        "        shape_slim=dataset_list[0].shape_slim,\n",
        "        pixel_scales=dataset_list[0].pixel_scales,\n",
        "    )\n",
        "\n",
        "    mask = ac.Mask1D.masked_fpr_and_eper_from(\n",
        "        mask=mask,\n",
        "        layout=dataset_list[0].layout,\n",
        "        settings=ac.SettingsMask1D(fpr_pixels=(0, 10)),\n",
        "        pixel_scales=dataset_list[0].pixel_scales,\n",
        "    )\n",
        "\n",
        "    dataset_list = [dataset.apply_mask(mask=mask) for dataset in dataset_list]\n",
        "\n",
        "    \"\"\"\n",
        "    __Info__\n",
        "\n",
        "    Information about our model-fit that isn't part of the model-fit can be made accessible to the database, by \n",
        "    passing an `info` dictionary. \n",
        "\n",
        "    Below we load this info dictionary from an `info.json` file stored in each dataset's folder. This dictionary\n",
        "    contains the (hypothetical) injection voltage settings of the data, as if it were made via a charge injection.\n",
        "    \"\"\"\n",
        "    info_list = []\n",
        "\n",
        "    for norm in norm_list:\n",
        "        info_file = path.join(dataset_path, f\"norm_{int(norm)}\", \"info.json\")\n",
        "\n",
        "        with open(info_file) as json_file:\n",
        "            info = json.load(json_file)\n",
        "\n",
        "        info_list.append(info)\n",
        "\n",
        "    \"\"\"\n",
        "    __Model__\n",
        "    \n",
        "    Set up the model as per usual.\n",
        "    \"\"\"\n",
        "    trap_0 = af.Model(ac.TrapInstantCapture)\n",
        "    trap_1 = af.Model(ac.TrapInstantCapture)\n",
        "\n",
        "    # Bug means cant combine session with assertion for now.\n",
        "\n",
        "    trap_0.add_assertion(trap_0.release_timescale < trap_1.release_timescale)\n",
        "\n",
        "    trap_list = [trap_0, trap_1]\n",
        "\n",
        "    ccd = af.Model(ac.CCDPhase)\n",
        "    ccd.well_notch_depth = 0.0\n",
        "    ccd.full_well_depth = 200000.0\n",
        "\n",
        "    model = af.Collection(cti=af.Model(ac.CTI1D, trap_list=trap_list, ccd=ccd))\n",
        "\n",
        "    \"\"\"\n",
        "    __Clocker / arCTIc__\n",
        "\n",
        "    Set up the clocker as per usual.\n",
        "    \"\"\"\n",
        "    clocker = ac.Clocker1D(express=5)\n",
        "\n",
        "    \"\"\"\n",
        "    In all examples so far, results were written to the `autofit_workspace/output` folder with a path and folder \n",
        "    named after a unique identifier, which was derived from the non-linear search and model. This unique identifier\n",
        "    plays a vital role in the database: it is used to ensure every entry in the database is unique. \n",
        "\n",
        "    In this example, results are written directly to the `database.sqlite` file after the model-fit is complete and \n",
        "    only stored in the output folder during the model-fit. This can be important for performing large model-fitting \n",
        "    tasks on high performance computing facilities where there may be limits on the number of files allowed, or there\n",
        "    are too many results to make navigating the output folder manually feasible.\n",
        "\n",
        "    The `unique_tag` below uses the `dataset_name` to alter the unique identifier, which as we have seen is also \n",
        "    generated depending on the search settings and model. In this example, all three model fits use an identical \n",
        "    search and model, so this `unique_tag` is key for ensuring 3 separate sets of results for each model-fit are \n",
        "    stored in the output folder and written to the .sqlite database. \n",
        "    \"\"\"\n",
        "    search = af.Nautilus(\n",
        "        path_prefix=path.join(\"database\"),\n",
        "        name=\"database_example\",\n",
        "        unique_tag=f\"{dataset_name}_{i}\",  # This makes the unique identifier use the dataset name\n",
        "        session=session,  # This can instruct the search to write to the .sqlite database.\n",
        "        n_live=100,\n",
        "    )\n",
        "\n",
        "    analysis_list = [\n",
        "        ac.AnalysisDataset1D(dataset=dataset, clocker=clocker)\n",
        "        for dataset in dataset_list\n",
        "    ]\n",
        "    analysis = sum(analysis_list)\n",
        "\n",
        "    search.fit(analysis=analysis, model=model, info=info)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Building a Database File From an Output Folder__\n",
        "\n",
        "The fits above wrote the results to hard-disk in folders, not as an .sqlite database file. \n",
        "\n",
        "We build the database below, where the `database_name` corresponds to the name of your output folder and is also the \n",
        "name of the `.sqlite` database file that is created.\n",
        "\n",
        "If you are fitting a relatively small number of datasets (e.g. 10-100) having all results written to hard-disk (e.g. \n",
        "for quick visual inspection) and using the database for sample wide analysis is beneficial.\n",
        "\n",
        "We can optionally only include completed model-fits but setting `completed_only=True`.\n",
        "\n",
        "If you inspect the `output` folder, you will see a `database.sqlite` file which contains the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "database_name = \"database\"\n",
        "\n",
        "agg = af.Aggregator.from_database(\n",
        "    filename=f\"{database_name}.sqlite\", completed_only=False\n",
        ")\n",
        "\n",
        "agg.add_directory(directory=path.join(\"output\", database_name))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Writing Directly To Database__\n",
        "\n",
        "Results can be written directly to the .sqlite database file, skipping output to hard-disk entirely, by creating\n",
        "a session and passing this to the non-linear search.\n",
        "\n",
        "The code below shows how to do this, but it is commented out to avoid rerunning the non-linear searches.\n",
        "\n",
        "This is ideal for tasks where model-fits to hundreds or thousands of datasets are performed, as it becomes unfeasible\n",
        "to inspect the results of all fits on the hard-disk. \n",
        "\n",
        "Our recommended workflow is to set up database analysis scripts using ~10 model-fits, and then scaling these up\n",
        "to large samples by writing directly to the database."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# session = af.db.open_database(\"database.sqlite\")\n",
        "#\n",
        "# search = af.Nautilus(\n",
        "#     path_prefix=path.join(\"database\"),\n",
        "#     name=\"database_example\",\n",
        "#     unique_tag=dataset_name,  # This makes the unique identifier use the dataset name\n",
        "#     session=session,  # This can instruct the search to write to the .sqlite database.\n",
        "#     n_live=100,\n",
        "# )"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Generators__\n",
        "\n",
        "Before using the aggregator to inspect results, let me quickly cover Python generators. A generator is an object that \n",
        "iterates over a function when it is called. The aggregator creates all of the objects that it loads from the database \n",
        "as generators (as opposed to a list, or dictionary, or other Python type).\n",
        "\n",
        "Why? Because lists and dictionaries store every entry in memory simultaneously. If you fit many datasets, this will use \n",
        "a lot of memory and crash your laptop! On the other hand, a generator only stores the object in memory when it is used; \n",
        "Python is then free to overwrite it afterwards. Thus, your laptop won't crash!\n",
        "\n",
        "There are two things to bare in mind with generators:\n",
        "\n",
        " 1) A generator has no length and to determine how many entries it contains you first must turn it into a list.\n",
        "\n",
        " 2) Once we use a generator, we cannot use it again and need to remake it. For this reason, we typically avoid \n",
        " storing the generator as a variable and instead use the aggregator to create them on use.\n",
        "\n",
        "We can now create a `samples` generator of every fit. The `results` example scripts show how  \n",
        "the `Samples` class acts as an interface to the results of the non-linear search."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "samples_gen = agg.values(\"samples\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When we print this the length of this generator converted to a list of outputs we see 3 different `SamplesNest`\n",
        "instances. \n",
        "\n",
        "These correspond to each fit of each search to each of our 3 images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"NestedSampler Samples: \\n\")\n",
        "print(samples_gen)\n",
        "print()\n",
        "print(\"Total Samples Objects = \", len(agg), \"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Therefore, by loading the `Samples` via the database we can now access the results of the fit to each dataset.\n",
        "\n",
        "For example, we can plot the maximum likelihood model for each of the 3 model-fits performed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ml_vector = [\n",
        "    samps.max_log_likelihood(as_instance=False) for samps in agg.values(\"samples\")\n",
        "]\n",
        "\n",
        "print(\"Max Log Likelihood Model Parameter Lists: \\n\")\n",
        "print(ml_vector, \"\\n\\n\")\n",
        "\n",
        "# \"\"\"\n",
        "# __Building a Database File From an Output Folder__\n",
        "#\n",
        "# The fits above directly wrote the results to the .sqlite file, which we loaded above. However, you may have results\n",
        "# already written to hard-disk in an output folder, which you wish to build your .sqlite file from.\n",
        "#\n",
        "# This can be done via the following code, which is commented out below to avoid us deleting the existing .sqlite file.\n",
        "#\n",
        "# Below, the `database_name` corresponds to the name of your output folder and is also the name of the `.sqlite` file\n",
        "# that is created.\n",
        "#\n",
        "# If you are fitting a relatively small number of datasets (e.g. 10-100) having all results written\n",
        "# to hard-disk (e.g. for quick visual inspection) but using the database for sample-wide analysis may be benefitial.\n",
        "# \"\"\"\n",
        "# database_name = \"database\"\n",
        "#\n",
        "# agg = af.Aggregator.from_database(\n",
        "#    filename=f\"{database_name}.sqlite\", completed_only=False\n",
        "# )\n",
        "#\n",
        "# agg.add_directory(directory=path.join(\"output\", database_name))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Wrap Up__\n",
        "\n",
        "This example illustrates how to use the database.\n",
        "\n",
        "The `database/examples` folder contains examples illustrating the following:\n",
        "\n",
        "- ``samples.py``: Loads the non-linear search results from the SQLite3 database and inspect the \n",
        "   samples (e.g. parameter estimates, posterior).\n",
        "   \n",
        "- ``queries.py``: Query the database to get certain modeling results (e.g. all models where `\n",
        "   einstein_radius > 1.0`).\n",
        "\n",
        "- ``models.py``: Inspect the models in the database (e.g. visualize their deflection angles).\n",
        "\n",
        "- ``data_fitting.py``: Inspect the data-fitting results in the database (e.g. visualize the residuals)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}